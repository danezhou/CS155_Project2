{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, ai, bj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    reg_grad = Ui * reg - (Yij - (np.dot(Ui, Vj) + ai + bj)) * Vj\n",
    "    return eta * reg_grad\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, ai, bj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    reg_grad = Vj * reg - (Yij - (np.dot(Vj, Ui) + ai + bj)) * Ui\n",
    "    return eta * reg_grad\n",
    "\n",
    "def grad_a(ai, Yij, Ui, Vj, bj, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the bias term ai, a training point Yij, Ui (the ith row of U),\n",
    "    column vector Vj (jth column of V^T), the bias term bj, and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to ai multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * -(Yij - (np.dot(Vj, Ui) + ai + bj))\n",
    "\n",
    "def grad_b(bj, Yij, Ui, Vj, ai, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the bias term bj, a training point Yij, Ui (the ith row of U),\n",
    "    column vector Vj (jth column of V^T), the bias term ai, and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to bj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * -(Yij - (np.dot(Vj, Ui) + ai + bj))\n",
    "\n",
    "\n",
    "def get_err(U, V, a, b, Y, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    reg_term = reg * (np.linalg.norm(U, 'fro')**2 + np.linalg.norm(V, 'fro')**2) / 2\n",
    "    \n",
    "    least_square_sum = 0\n",
    "    \n",
    "    for (i, j, Yij) in Y:\n",
    "        Ui = U[i - 1]\n",
    "        Vj = V[j - 1]\n",
    "        ai = a[i - 1]\n",
    "        bj = b[j - 1]\n",
    "        least_square_sum += (Yij - (np.dot(Ui, Vj) + ai + bj))**2\n",
    "    \n",
    "    least_square_sum /= 2\n",
    "    \n",
    "    return (reg_term + least_square_sum) / Y.shape[0]\n",
    "\n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij. Also learns bias terms a and b for users and movies.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, a, b, err) consisting of U, V, a, b and the\n",
    "    unregularized MSE of the model.\n",
    "    \"\"\"\n",
    "    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n",
    "    a = np.random.uniform(-0.5, 0.5, size=(M))\n",
    "    b = np.random.uniform(-0.5, 0.5, size=(N))\n",
    "    initial_delta = 0\n",
    "    old_loss = get_err(U, V, a, b, Y, reg)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        indices = np.random.permutation(len(Y))\n",
    "        \n",
    "        for index in indices:\n",
    "            (i, j, Yij) = Y[index]\n",
    "            Ui = U[i - 1]\n",
    "            Vj = V[j - 1]\n",
    "            ai = a[i - 1]\n",
    "            bj = b[j - 1]\n",
    "            new_Ui = Ui - grad_U(Ui, Yij, Vj, ai, bj, reg, eta)\n",
    "            new_Vj = Vj - grad_V(Vj, Yij, Ui, ai, bj, reg, eta)\n",
    "            new_ai = ai - grad_a(ai, Yij, Ui, Vj, bj, eta)\n",
    "            new_bj = bj - grad_b(bj, Yij, Ui, Vj, ai, eta)\n",
    "            U[i - 1] = new_Ui\n",
    "            V[j - 1] = new_Vj\n",
    "            a[i - 1] = new_ai\n",
    "            b[j - 1] = new_bj\n",
    "        \n",
    "        new_loss = get_err(U, V, a, b, Y, reg)\n",
    "            \n",
    "        if epoch == 0:\n",
    "            initial_delta = new_loss - old_loss\n",
    "        else:\n",
    "            delta = new_loss - old_loss\n",
    "            if abs(delta / initial_delta) <= eps:\n",
    "                break\n",
    "        \n",
    "        old_loss = new_loss\n",
    "        \n",
    "    err = get_err(U, V, a, b, Y)\n",
    "    return (U, V, a, b, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  943  users,  1682  movies.\n",
      "0.2619575857470955 0.4158797841374228\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.loadtxt('./data/train.txt').astype(int)\n",
    "Y_test = np.loadtxt('./data/test.txt').astype(int)\n",
    "\n",
    "# data is 1 indexed.\n",
    "\n",
    "M = max(max(Y_train[:,0]), max(Y_test[:,0])).astype(int) # users\n",
    "N = max(max(Y_train[:,1]), max(Y_test[:,1])).astype(int) # movies\n",
    "print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "\n",
    "reg = 0.1\n",
    "eta = 0.03 # learning rate\n",
    "K = 20\n",
    "\n",
    "# Use to compute Ein and Eout\n",
    "U, V, a, b, err = train_model(M, N, K, eta, reg, Y_train)\n",
    "E_in = err\n",
    "E_out = get_err(U, V, a, b, Y_test)\n",
    "print(E_in, E_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
