{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    reg_grad = Ui * reg - (Yij - np.dot(Ui, Vj)) * Vj\n",
    "    return eta * reg_grad\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    reg_grad = Vj * reg - (Yij - np.dot(Vj, Ui)) * Ui\n",
    "    return eta * reg_grad\n",
    "\n",
    "def get_err(U, V, Y, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    reg_term = reg * (np.linalg.norm(U, 'fro')**2 + np.linalg.norm(V, 'fro')**2) / 2\n",
    "    \n",
    "    least_square_sum = 0\n",
    "    \n",
    "    for (i, j, Yij) in Y:\n",
    "        Ui = U[i - 1]\n",
    "        Vj = V[j - 1]\n",
    "        least_square_sum += (Yij - np.dot(Ui, Vj))**2\n",
    "    \n",
    "    least_square_sum /= 2\n",
    "    \n",
    "    return (reg_term + least_square_sum) / Y.shape[0]\n",
    "\n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n",
    "    initial_delta = 0\n",
    "    old_loss = get_err(U, V, Y, reg)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        indices = np.random.permutation(len(Y))\n",
    "        \n",
    "        for index in indices:\n",
    "            (i, j, Yij) = Y[index]\n",
    "            Ui = U[i - 1]\n",
    "            Vj = V[j - 1]\n",
    "            new_Ui = Ui - grad_U(Ui, Yij, Vj, reg, eta)\n",
    "            new_Vj = Vj - grad_V(Vj, Yij, Ui, reg, eta)\n",
    "            U[i - 1] = new_Ui\n",
    "            V[j - 1] = new_Vj\n",
    "        \n",
    "        new_loss = get_err(U, V, Y, reg)\n",
    "            \n",
    "        if epoch == 0:\n",
    "            initial_delta = new_loss - old_loss\n",
    "        else:\n",
    "            delta = new_loss - old_loss\n",
    "            if abs(delta / initial_delta) <= eps:\n",
    "                break\n",
    "        \n",
    "        old_loss = new_loss\n",
    "        \n",
    "    err = get_err(U, V, Y)\n",
    "    return (U, V, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  943  users,  1664  movies.\n",
      "0.30580711741361805 0.45159062027214625\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.loadtxt('./data/train_parsed.txt').astype(int)\n",
    "Y_test = np.loadtxt('./data/test_parsed.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  943  users,  1664  movies.\n",
      "-------------------------------------\n",
      "Training with reg = 0.050000 and eta = 0.005000\n",
      "0.20568971003949055 0.4471542098375749\n",
      "-------------------------------------\n",
      "Training with reg = 0.050000 and eta = 0.010000\n",
      "0.22504301665174356 0.44851741690030095\n",
      "-------------------------------------\n",
      "Training with reg = 0.050000 and eta = 0.030000\n",
      "0.21791409042128637 0.4765569554240941\n",
      "-------------------------------------\n",
      "Training with reg = 0.050000 and eta = 0.050000\n",
      "0.24780495679744827 0.49466532794746726\n",
      "-------------------------------------\n",
      "Training with reg = 0.100000 and eta = 0.005000\n",
      "0.2875192371826772 0.41989275063126413\n",
      "-------------------------------------\n",
      "Training with reg = 0.100000 and eta = 0.010000\n",
      "0.29943304841697027 0.428454776119973\n",
      "-------------------------------------\n",
      "Training with reg = 0.100000 and eta = 0.030000\n",
      "0.3281978645555297 0.4513196555646375\n",
      "-------------------------------------\n",
      "Training with reg = 0.100000 and eta = 0.050000\n",
      "0.32962780905225986 0.4788533426178682\n",
      "-------------------------------------\n",
      "Training with reg = 0.150000 and eta = 0.005000\n",
      "0.3679810964497909 0.43666397716602134\n",
      "-------------------------------------\n",
      "Training with reg = 0.150000 and eta = 0.010000\n",
      "0.3824344250715977 0.449727985376398\n",
      "-------------------------------------\n",
      "Training with reg = 0.150000 and eta = 0.030000\n",
      "0.38093016373545674 0.45790768018176675\n",
      "-------------------------------------\n",
      "Training with reg = 0.150000 and eta = 0.050000\n",
      "0.4365610803936617 0.49273137930348093\n",
      "-------------------------------------\n",
      "Training with reg = 0.200000 and eta = 0.005000\n",
      "0.4143108485102019 0.46148821794004014\n",
      "-------------------------------------\n",
      "Training with reg = 0.200000 and eta = 0.010000\n",
      "0.4353299810070855 0.4844138767024028\n",
      "-------------------------------------\n",
      "Training with reg = 0.200000 and eta = 0.030000\n",
      "0.4465540504158203 0.4910947823192876\n",
      "-------------------------------------\n",
      "Training with reg = 0.200000 and eta = 0.050000\n",
      "0.46630974629845345 0.5122949365556245\n",
      "-------------------------------------\n",
      "Training with reg = 0.250000 and eta = 0.005000\n",
      "0.4403022054142021 0.4778069754861082\n",
      "-------------------------------------\n",
      "Training with reg = 0.250000 and eta = 0.010000\n",
      "0.4590065103619046 0.49644252134056976\n",
      "-------------------------------------\n",
      "Training with reg = 0.250000 and eta = 0.030000\n",
      "0.4769729087398633 0.5107523203083514\n",
      "-------------------------------------\n",
      "Training with reg = 0.250000 and eta = 0.050000\n",
      "0.4991239517076266 0.5300730101889541\n"
     ]
    }
   ],
   "source": [
    "# data is 1 indexed.\n",
    "M = max(max(Y_train[:,0]), max(Y_test[:,0])).astype(int) # users\n",
    "N = max(max(Y_train[:,1]), max(Y_test[:,1])).astype(int) # movies\n",
    "print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "\n",
    "regs = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "etas = [0.005, 0.01, 0.03, 0.05] # learning rate\n",
    "K = 20\n",
    "\n",
    "# Use to compute Ein and Eout and do hyperparameter tuning\n",
    "for reg in regs:\n",
    "    for eta in etas:\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"Training with reg = %f and eta = %f\" % (reg, eta))\n",
    "        U,V, err = train_model(M, N, K, eta, reg, Y_train)\n",
    "        E_in = err\n",
    "        E_out = get_err(U, V, Y_test)\n",
    "        print(E_in, E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33533442726563173\n"
     ]
    }
   ],
   "source": [
    "# reg 0.1 and eta 0.005 had best generalization\n",
    "\n",
    "data = np.loadtxt('./data/data_parsed.txt').astype(int)\n",
    "\n",
    "reg = 0.1\n",
    "eta = 0.005\n",
    "\n",
    "U,V, err = train_model(M, N, K, eta, reg, data)\n",
    "E_in = err\n",
    "print(E_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
